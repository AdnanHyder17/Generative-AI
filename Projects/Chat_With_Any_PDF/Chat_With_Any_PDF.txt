ğŸš€ Project Launch: Chat with Any PDF using LangChain, FAISS, Streamlit & LLaMA 3.2 ğŸ“„ğŸ’¬

I recently built a full-stack AI-powered PDF chatbot that allows users to upload multiple PDFs and ask natural language questions about their contents â€” all within an interactive Streamlit app.

ğŸ” Project Overview:
This project enables Retrieval-Augmented Generation (RAG) over PDF documents. Whether it's contracts, research papers, invoices, or manuals â€” users can ask questions and get accurate, cited responses from the uploaded documents in real time.

ğŸ’¡ Key Features:
ğŸ“¤ Upload multiple PDF files at once
ğŸ§  Automatically processes and chunks documents into manageable text segments
ğŸ” Uses FAISS to create a vector store for fast similarity-based retrieval
ğŸ¤– Leverages Ollamaâ€™s LLaMA 3.2 LLM to generate responses using LangChain
ğŸ§© Integrates LangChain RAG components for context-aware question answering
ğŸ§¾ Displays source documents and page numbers for transparency
ğŸ’¬ Maintains chat history for ongoing multi-turn conversations

ğŸ§° Tech Stack:
LangChain: Document loaders, text splitting, and chaining logic
FAISS: Fast and efficient vector similarity search
Ollama (LLaMA 3.2): Local language model for answering queries
Streamlit: UI for seamless interaction and chat interface
PyMuPDF: For reading and parsing PDF content

ğŸŒŸ What I Learned:
Building end-to-end pipelines using LangChain Runnables for flexible chaining
Working with local LLMs (LLaMA 3.2) through Ollama â€” enabling private, offline inference
Handling multi-PDF ingestion, metadata tagging, and chunk-based retrieval
Creating a responsive and user-friendly app with Streamlitâ€™s session state management






ğŸš€ğŸš€ Complete Setup Instructions for Chat with Any PDF Project
ğŸ’¡ Step 1: Activate Virtual Environment
Open terminal in your project folder and run: venv\Scripts\activate

ğŸ“¤ Step 2: Download Ollama
Go to https://ollama.ai
Download and install Ollama for Windows
Complete the installation then restart the terminal

ğŸ§  Step 3: Pull Required Models
Open a new terminal and run these commands one by one: 
- ollama pull llama3.2:1b
- ollama pull nomic-embed-text

ğŸ” Step 4: Start Ollama Server
Keep a terminal open and run: ollama serve

ğŸ¤– Step 5: Run the Streamlit Application
Open a new terminal in your project folder and run: streamlit run app.py

ğŸ§© Step 6: Use the Application
1- Click "Browse files" to upload one or more PDF documents
2- Click the "Process Documents" button
3- Wait for the success message showing number of chunks processed
4- Type your question in the text input box
5- Wait for the answer to generate
6- Click "View Source Documents" to see which parts of the PDF were used
7- Your chat history is saved and displayed below
8- Use the sidebar to clear chat history or select different models